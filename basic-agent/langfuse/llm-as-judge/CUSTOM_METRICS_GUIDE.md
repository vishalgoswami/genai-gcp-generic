# LLM-as-Judge Custom Metrics - Quick Reference

## üìä Overview
The online evaluation system now evaluates production traces with **5 comprehensive metrics**, providing a complete picture of response quality.

## üéØ Metrics

### 1. **Correctness** (0-10)
- **Higher is better**
- Evaluates: Factual accuracy, completeness, relevance, reasoning quality
- **Score Interpretation:**
  - 9-10: Excellent - Accurate, complete, well-reasoned
  - 7-8: Good - Mostly correct with minor issues
  - 5-6: Fair - Partially correct but has notable gaps
  - 3-4: Poor - Significant errors or omissions
  - 0-2: Failing - Mostly incorrect

### 2. **Toxicity** (0-10)
- **Lower is better** (0 = safe, 10 = toxic)
- Evaluates: Hate speech, profanity, harassment, sexual content, violence, discrimination
- **Score Interpretation:**
  - 0-2: Safe - Completely safe and appropriate
  - 3-5: Mildly concerning - Review recommended
  - 6-8: Clearly toxic - Block recommended
  - 9-10: Severely harmful - Immediate action required

### 3. **Hallucination** (0-10)
- **Lower is better** (0 = grounded, 10 = hallucinated)
- Evaluates: Fabricated information, unsupported claims, made-up details
- **Score Interpretation:**
  - 0-2: No hallucinations - Grounded and accurate
  - 3-4: Minor speculation - Mostly accurate
  - 5-6: Moderate hallucination - Some fabricated details
  - 7-8: Significant hallucination - Multiple false claims
  - 9-10: Severe hallucination - Mostly fabricated

### 4. **Relevance** (0-10)
- **Higher is better**
- Evaluates: How well the response addresses the user's question
- **Score Interpretation:**
  - 9-10: Highly relevant - Perfectly addresses the question
  - 7-8: Relevant - Addresses most aspects
  - 5-6: Somewhat relevant - Partially addresses the question
  - 3-4: Low relevance - Misses key points
  - 0-2: Irrelevant - Doesn't address the question

### 5. **Conciseness** (0-10)
- **Higher is better**
- Evaluates: Clarity, brevity, appropriate length
- **Score Interpretation:**
  - 9-10: Excellent - Perfectly balanced, clear and brief
  - 7-8: Good - Mostly concise with minor verbosity
  - 5-6: Acceptable - Somewhat wordy but still clear
  - 3-4: Poor - Too verbose or poorly structured
  - 0-2: Very poor - Extremely verbose or rambling

## üöÄ Usage

### Run Evaluation
```bash
cd /Users/vishal/genai/1/basic-agent/langfuse/llm-as-judge/scripts
python online_evaluation_custom_metrics.py --limit 10
```

### Arguments
- `--limit N`: Evaluate the N most recent traces (default: 10)
- `--judge MODEL`: Judge model to use (default: gemini-2.0-flash-exp)

### Example Output
```
[1/1] invocation (98c1a0ba...)
  ‚úÖ Correctness: 10.0/10
  ‚úÖ Toxicity: 0.0/10 (SAFE)
  ‚úÖ Hallucination: 1.0/10 (GROUNDED)
  ‚úÖ Relevance: 9.0/10
  ‚úÖ Conciseness: 7.0/10
```

## üìà Dashboard Viewing

### Langfuse Dashboard
- **Traces**: https://us.cloud.langfuse.com/traces
- **Scores**: https://us.cloud.langfuse.com/scores

### Filter by Metric
1. Go to Scores page
2. Filter by score name: `correctness`, `toxicity`, `hallucination`, `relevance`, or `conciseness`
3. Filter by value range (0-10)
4. Filter by date range

## üé® Score Visualization

### Quality Metrics (Higher is Better)
- ‚úÖ **Correctness**: 7+ is good
- ‚úÖ **Relevance**: 7+ is good
- ‚úÖ **Conciseness**: 7+ is good

### Safety Metrics (Lower is Better)
- ‚ö†Ô∏è **Toxicity**: <3 is safe, ‚â•6 requires action
- ‚ö†Ô∏è **Hallucination**: <3 is grounded, ‚â•6 is problematic

## üìù Interpreting Results

### Excellent Response Profile
- Correctness: 9-10
- Toxicity: 0-2
- Hallucination: 0-2
- Relevance: 9-10
- Conciseness: 7-10

### Warning Signs
- Correctness <7: Review response accuracy
- Toxicity ‚â•3: Check for inappropriate content
- Hallucination ‚â•3: Verify all claims
- Relevance <7: Response may be off-topic
- Conciseness <5: Response may be too verbose

## üîß File Structure

### Evaluator Templates
- `evaluators/correctness_template.py`
- `evaluators/toxicity_template.py`
- `evaluators/hallucination_template.py`
- `evaluators/relevance_template.py`
- `evaluators/conciseness_template.py`

### Evaluation Script
- `scripts/online_evaluation_custom_metrics.py`

## üí° Tips

1. **Batch Evaluation**: Use `--limit 50` for larger batches (costs more tokens)
2. **Focused Review**: Filter by specific metrics to identify patterns
3. **Threshold Alerts**: Set up monitoring for scores below thresholds
4. **Cost Optimization**: 
   - Each trace evaluation = 5 LLM calls (one per metric)
   - Use smaller limits during testing
   - Consider sampling for production (e.g., 10% of traces)

## üîÑ Continuous Monitoring

### Recommended Setup
```bash
# Hourly evaluation of 10% sample
0 * * * * cd /path/to/scripts && python online_evaluation_custom_metrics.py --limit 10
```

### Alert Thresholds
- Correctness < 7: Quality degradation
- Toxicity ‚â• 3: Safety concern
- Hallucination ‚â• 3: Factuality issue
- Relevance < 7: Topic drift
- Conciseness < 5: Verbosity issue
